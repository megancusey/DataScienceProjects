rsq(log(dtrain$PINCP,base=10), predict(model, newdata=dtrain))
## R-Squared of Test Sest
rsq(log(dtest$PINCP,base=10),predict(model, newdata=dtest))
## RMSE
<- rmse <- function(y,f) {sqrt(mean((y-f)^2))}
## RMSE
rmse <- function(y,f) {sqrt(mean((y-f)^2))}
## RMSE of the Training Data Set
rmse(log(dtrain$PINCP, base=10), predict(model, newdata=dtrain))
##[1] 0.2651856
## RMSE of the Test Data Set
rmse(log(dtest$PINCP, base=10), predict(model,newdata=dtest))
## What is the value of having a bachelor's degree?
## Coefficients measure the value of a independent variable's
## effect on the dependent variable.
coefficients(model)
## ARE THE COEFFICIENTS RELIABLE?
summary(model)
rm()
rm(list = ls())
load("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B/NatalRiskData.rData")
## Subset into test/train
train <- sdata[sdata$ORIGRANDGROUP <=5,]
test <- sdata[sdata$ORIGRANDGROUP >5,]
## BUILD MODEL
complications <- c("ULD_MECO", "ULD_PRECIP","ULD_BREECH")
riskfactors <- c("URF_DIAB","URF_CHYPER","URF_PHYPER","URF_ECLAM")
y <- "atRisk"
x <- c("PWGT", "UPREVIS","CIG_REC","GESTREC3","DPLURAL", complications, riskfactors)
## paste() concatencate after converting to character
fmla <- paste(y, paste(x, collapse = "+"), sep="~")
## FIT LOGISTIC REGRESSION MODEL
print(fmla)
model <- glm(fmla, data=train, family=binomial(link="logit"))
## MAKE PREDICTIONS
train$pred <- predict(model, newdata=train, type="response")
test$pred <- predict(model, newdata=test, type="response")
ggplot(train, aes(x=pred, color=atRisk, linetype=atRisk)) +
geom_density()
install.packages(ROCR)
install.packages("ROCR")
##install.packages("ROCR")
library(ROCR)
##install.packages("ROCR")
install.packages("grid")
install.packages("grid")
install.packages("grid")
library(gride)
library(grid)
predObj <- prediction(train$pred,train$atRisk)
##install.packages("ROCR")
##install.packages("grid")
library(ROCR)
install.packages(c("backports", "curl", "digest", "ellipsis", "ggpubr", "prodlim", "R6", "Rcpp", "RcppArmadillo", "rlang", "TTR"))
##install.packages("ROCR")
##install.packages("grid")
library(ROCR)
library(grid)
predObj <- prediction(train$pred,train$atRisk)
predObj <- prediction(train$pred,train$atRisk)
predObj <- prediction(train$pred,train$atRisk)
precObj <- performance(predObj, measure="prec")
recObj <- performance(predObj, measure="rec")
precision <-(precObj@y.values)[[1]]
prec.x <-(precObj@x.values)[[1]]
recall <-(recObj@y.values)[[1]]
rocFrame <- data.frame(threshold=prec.x, precision=precision, recall=recall)
nplot <- function(plist){
n<- length(plist)
grid.newpage()
pushViewport(viewport(layout=grid.layout(n,1)))
vplayout=function(x,y) {viewport(layout.pos.row = x, layout.pos.col = y)}
for(i in 1:n) {
print(plist[[i]], vp=vplayout(i,1))
}
}
pnull <- mean(as.numeric(train$atRisk))
## THIS FUNCTION PLOTS STACKED PLOTS ON A SINGLE PAGE
nplot <- function(plist){
n<- length(plist)
grid.newpage()
pushViewport(viewport(layout=grid.layout(n,1)))
vplayout=function(x,y) {viewport(layout.pos.row = x, layout.pos.col = y)}
for(i in 1:n) {
print(plist[[i]], vp=vplayout(i,1))
}
}
pnull <- mean(as.numeric(train$atRisk))
## PLOT ENRICHMENT RATE AS A F(X) OF THRESHOLD
p1 <-ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim = c(0,10))
library(ggplot2)
## PLOT ENRICHMENT RATE AS A F(X) OF THRESHOLD
p1 <-ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim = c(0,10))
## plot recall as F(X) of Threshold
p2 <- ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=recall)) +
coord_cartesian(xlim=c(0,0.05))
nplot(list(p1,p2))
ctab.test <- table(pred=test$pred > .02, atRisk=test$atRisk)
ctab.test
precision <- ctab.test[2,2]/sum(ctab.test[2,])
precision
recall <-ctab.test[2,2]/sum(ctab.test[,2])
recall
enrich <- precision(mean(as.numeric(test$atRisk)))
enrich <- precision/mean(as.numeric(test$atRisk))
enrich
## FINDING RELATIONS AND EXTRACTING ADVICE:
coefficients(model)
summary(model)
## CALCULATING DEVIANCE RESIDUALS
## CREATE VECTOR OF PREDICTIONS FOR TRAINING DATA
pred <- predict(model, newdata=train, type="response")
llcomponents <- function(y, py) {
y*log(py) + (1-y) * log(1-py)
}
edev <- sign(as.numeric(train$atRisk) - pred *
sqrt(-2*llcomponents(as.numeric(train$atRisk),pred)))
summary(edev)
edev <- sign(as.numeric(train$atRisk) - pred) *
sqrt(-2*llcomponents(as.numeric(train$atRisk),pred))
summary(edev)
## CREATE A FUNCTION TO CALC THE LOG LIKELIHOOK OF A DATASET.
## Y = OUTCOME IN NUMERIC FORM, PY PREDICTED PROBABILITY THAT Y =1.
loglikelihood <- function(y,py) {
sum(y*log(py) + (1-y)*log(1-py))
}
pnull <- mean(as.numeric(train$atRisk))
null.dev <- 2*loglikelihood(as.numeric(train$atRisk),pnull)
pnull
## [1] 0.01920912
null.dev
## [1] -2698.716 why did I get negative here?
model$null.deviance
## [1] 2698.716
pred <- predict(model, newdata=train, type="response")
resid.dev
resid.dev <- 2*loglikelihood(as.numeric(train$atRisk),pred)
resid.dev
## CREATE A FUNCTION TO CALC THE LOG LIKELIHOOK OF A DATASET.
## Y = OUTCOME IN NUMERIC FORM, PY PREDICTED PROBABILITY THAT Y =1.
loglikelihood <- function(y,py) {
sum(y*log(py) + (1-y)*log(1-py))
}
pnull <- mean(as.numeric(train$atRisk))
null.dev <- 2*loglikelihood(as.numeric(train$atRisk),pnull)
pnull
## [1] 0.01920912
null.dev
## [1] -2698.716 why did I get negative here?
model$null.deviance
## [1] 2698.716
pred <- predict(model, newdata=train, type="response")
resid.dev <- 2*loglikelihood(as.numeric(train$atRisk),pred)
resid.dev
model$deviance
pred <- predict(model, newdata=train, type="response")
##[1] 2462.992
testy <- as.numeric(test$atRisk)
testpred<-predict(model,newdata=test,type="response")
pnull.test <-mean(testy)
null.dev.test <- 2*loglikelihood(testy, pnull.test)
null.dev.test <- -2*loglikelihood(testy, pnull.test)
resid.dev.test <- -2*loglikelihood(testy,testpred)
pnull.test
## [1] 0.0172713
null.dev.test
## [1] 2110.91
resid.dev.test
rm(list = ls())
##rm(list = ls())
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
kings
ts-kings <- ts(kings)
tskings <- ts(kings)
tskings
plot.ts(tskings)
install.packages("TTR")
##install.packages("TTR")
library(TTR)
##install.packages("TTR")
library("TTR")
## Use a simple moving average to
## smooth the data (order 3)
tskings.sma <- sma(tskings, n=3)
## Use a simple moving average to
## smooth the data (order 3)
tskings.sma <- SMA(tskings, n=3)
plot.ts(tskings.sma)
tskings.sma <- SMA(tskings, n=8)
plot.ts(tskings.sma)
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
##rm(list = ls())
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
rm(list = ls())
##rm(list = ls())
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
tskings <- ts(kings)
tskings
plot.ts(tskings)
##install.packages("TTR")
library("TTR")
## Use a simple moving average to
## smooth the data (order 3)
tskings.sma <- SMA(tskings, n=3)
plot.ts(tskings.sma)
tskings.sma <- SMA(tskings, n=8)
plot.ts(tskings.sma)
tskings.diff1 <- diff(tskings, differences=1)
plot.ts(tskings.diff1)
##ACF
acf(tskings.diff1,lag.max=20)
## GET VALUES ONLY
acf(tskings.diff1,lag.max = 20, plot=FALSE)
##PACF
pacf(tskings.diff1, lag.max=20)
pacf(tskings.diff1, lag.max=20, plot=FALSE)
tskings.arima <- arima(tskings, order=c(0,1,1))
tskings.arima
## Forecast
library("forecast")
tskings.forecasts <- forecast.Arima(tskings.arima, h = 5)
tskings.forecasts <- forecast(tskings.arima, h = 5)
tskings.forecasts
## PLOT FORECASTS
plot.forecasts(tskings.forecasts)
## PLOT FORECASTS
plot(tskings.forecasts)
## Check out residuals
acf(tskings.forecasts$residuals,lag.max=20)
Box.test(tskings.forecasts$residuals, lag=20, type="Ljung-Box")
plot.ts(tskings.forecasts$residuals)
plot.ts(tskings.forecasts$residuals)
plotForecastErrors(tskings.forecasts$residuals)
barplot(tskings.forecasts$residuals)
plot(tskings.forecasts$residuals)
rm(list = ls())
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
## SECTION 1 - data preprocessing
## OBJECTIVE - clean each feature according to data definitions
source("data preprocessing.R", local = TRUE)
options(scipen = 999)
data <- read.csv(file='train.csv', header=TRUE, stringsAsFactors = FALSE)
data_raw <- data
## cleaner - apply validations according to data definitions
data_clean <- data_cleaner(data_raw)
data <- data_clean
rm("data_raw")
## DATA EXPLORATION - See Data Exploration.R
source("data exploration.R", local = TRUE)
## feature engineering
source("feature engineering.R", local = TRUE)
## create a version of the data w/ feature selection applied.
data_FE <- featureEngineering(data_clean)
data <- data_FE
rm("data_clean")
rm("data_FE")
important_vars <- RFE(data)
k <- 8
keep_vars <- head(important_vars,k)
keep_vars <- c(keep_vars,"Target")
data_FS <- data[keep_vars]
train <- splitData("train", 0.8, data_FS)
test <- splitData("test", 0.8, data_FS)
accuracyMeasures <- function(pred, truth, name="model", prob){
## Normalizethe deviance by th number of data points so we can compare the deviance
## across the test and training sets
dev.norm <- -2*loglikelihood(as.numeric(truth),pred)/length(pred)
## Convert the class probability estimator into a classifer
## by labeling documents that score greater than .5 as spam.
ctable <- table(truth=truth,
pred=(pred > prob))
accuracy <- sum(diag(ctable))/sum(ctable)
precision <- ctable[2,2]/sum(ctable[,2])
recall <- ctable[2,2]/ sum(ctable[2,])
f1 <- precision*recall
data.frame(model=name, accuracy=accuracy, f1=f1, precision=precision,recall=recall, dev.norm)
}
## CALCULATE THE LOG LIKELIHOOD
loglikelihood <- function(y,py) {
pysmooth <- ifelse(py==0, 1e-12,
ifelse(py==1,1-1e-12,py))
sum(y*log(pysmooth) + (1-y)*log(1-pysmooth))
}
data <- data_FS
summary(data$Target)
data$HighDollar <- ifelse(data$Target >= 214000,"yes","no")
data$HighDollar <- factor(data$HighDollar)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(Target))
test <- subset(test, select =-c(Target))
train <- subset(train, select =-c(Target))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = HIGH DOLLAR HOME
houseFormula <- as.formula(paste('HighDollar=="yes"', paste(houseVars,collapse=" + "),sep=" ~ "))
pred_decision <- predict(treemodel, newdata=train)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
accuracyMeasures(predict(treemodel, newdata=train), train$HighDollar=="yes",  name="tree, training",.75)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
library(rpart)
library(rpart.plot)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
accuracyMeasures(predict(treemodel, newdata=train), train$HighDollar=="yes",  name="tree, training",.75)
## W/ Feature Engineering
## 1 tree, training 0.9203822 0.7096442 0.4059239
accuracyMeasures(predict(treemodel, newdata=test,.75),
test$HighDollar=="yes",
name="tree, test")
##[1] 0.9159068
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar=="yes",
name="baggin, training",.75)
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar=="yes",
name="bagging, test",.75)
## W/ Feature Engineering
## 1 tree, training 0.9203822 0.7096442 0.4059239
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar=="yes",
name="tree, test",.75)
## Use bootstrap samples that are the same size as the training set, with 100 trees.
ntrain <- dim(train)[1] ## returns # of rows in train (1258)
n <- ntrain
ntree <- 100
## BUILD THE BOOTSTRAP SAMPLES BY SAMPLING THE ROW INDICES OF TRAIN WITH REPLACEMENT
## EACH COLUMN OF THE MATRIX SAMPLES REPRESENT THE ROW INDICES INTO TRAIN THAT COMPRISE
## THE BOOTSTRAP SAMPLE.
samples <- sapply(1:ntree, ## loop 1-100
FUN = function(iter){
## obtain 100 samples, with replacement
sample(1:ntrain,
size=n,
replace=T)
})
## TRAIN THE INDIVIDUAL TREES FOR THE SAMPLES CREATED ABOVE AND RETURN THEM IN A LIST
treelist <- lapply(1:ntree,
FUN=function(iter){
samp <- samples[,iter];
rpart(houseFormula, train[samp,])
})
## Use predict.bag that assumes the underlying classifier to return decision probabilities
## instead of decisions
predict.bag <- function(treelist, newdata){
preds <- sapply(1:length(treelist),
FUN=function(iter) {
predict(treelist[[iter]], newdata=newdata)
})
predsums <- rowSums(preds)
predsums/length(treelist)
}
treelist
##[1] 0.9159068
accuracyMeasures(predict.bag(treelist, newdata=train),
train$HighDollar=="yes",
name="baggin, training",.75)
accuracyMeasures(predict.bag(treelist, newdata=test),
test$HighDollar=="yes",
name="bagging, test",.75)
library(randomForest)
set.seed(5123512)
f <- randomForest(x=data[1:1458,-9], y=data$HighDollar[1:1458], ntree=100,importance=TRUE)
fmodel <- randomForest(x=train[,houseVars], ## independent variables
y=train$HighDollar, ## dependent variable
ntree = 100, ## default = 500 but want 100 to compare to bagging example
nodesize=5, ## specifies that each node of a tree must have a min of 7 features
importance=T) ## tells function to save information for calculating variable importance
varImp <- importance(fmodel)
varImpPlot(fmodel, type=1)
plot( treelist )
library(ipred)
library(MASS)
bagging(HighDollar~.,data=train, coob=TRUE)
plot(x)
x <- bagging(HighDollar~.,data=train, coob=TRUE)
plot(x)
library(caret)
library(caret)
accuracy <- accuracy(ctable)
precision <- precision(ctable)
library(caret)
accuracyMeasures <- function(pred, truth, name="model", prob){
library(caret)
## Normalizethe deviance by th number of data points so we can compare the deviance
## across the test and training sets
dev.norm <- -2*loglikelihood(as.numeric(truth),pred)/length(pred)
## Convert the class probability estimator into a classifer
## by labeling documents that score greater than .5 as spam.
ctable <- table(truth=truth,
pred=(pred > prob))
accuracy <- sum(diag(ctable))/sum(ctable)
precision <- precision(ctable)
recall <-recall(ctable)
f1 <- precision*recall
data.frame(model=name, accuracy=accuracy, f1=f1, precision=precision,recall=recall, dev.norm)
}
accuracyMeasures(predict(treemodel, newdata=train), train$HighDollar=="yes",  name="tree, training",.75)
data.frame(model=name, accuracy=accuracy, f1=f1, precision=precision,recall=recall, dev.norm, cm=ctable)
accuracyMeasures <- function(pred, truth, name="model", prob){
library(caret)
## Normalizethe deviance by th number of data points so we can compare the deviance
## across the test and training sets
dev.norm <- -2*loglikelihood(as.numeric(truth),pred)/length(pred)
## Convert the class probability estimator into a classifer
## by labeling documents that score greater than .5 as spam.
ctable <- table(truth=truth,
pred=(pred > prob))
accuracy <- sum(diag(ctable))/sum(ctable)
precision <- precision(ctable)
recall <-recall(ctable)
f1 <- precision*recall
data.frame(model=name, accuracy=accuracy, f1=f1, precision=precision,recall=recall, dev.norm, cm=ctable)
}
## W/ Feature Engineering
## 1 tree, training 0.9203822 0.7096442 0.4059239
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar=="yes",
name="tree, test",.75)
accuracyMeasures <- function(pred, truth, name="model", prob){
library(caret)
## Normalizethe deviance by th number of data points so we can compare the deviance
## across the test and training sets
dev.norm <- -2*loglikelihood(as.numeric(truth),pred)/length(pred)
## Convert the class probability estimator into a classifer
## by labeling documents that score greater than .5 as spam.
ctable <- table(truth=truth,
pred=(pred > prob))
accuracy <- sum(diag(ctable))/sum(ctable)
precision <- precision(ctable)
recall <-recall(ctable)
f1 <- precision*recall
data.frame(model=name, accuracy=accuracy, f1=f1, precision=precision,recall=recall, dev.norm)
}
## Making the Confusion Matrix
cm_decision = table(y_pred_decision,train$HighDollar)
cm_decision
data <- data_FS
summary(data$Target)
data$HighDollar <- ifelse(data$Target >= 214000,"yes","no")
data$HighDollar <- factor(data$HighDollar)
split <- .8
train <- splitData("train", split ,data)
test <- splitData("test", split,data)
data <- subset(data, select =-c(Target))
test <- subset(test, select =-c(Target))
train <- subset(train, select =-c(Target))
houseVars <- setdiff(colnames(data), list("rgroup","HighDollar"))
houseVars
## USE ALL THE FEATURES AND DO BINARY CLASSIFICATION WHERE TRUE = HIGH DOLLAR HOME
houseFormula <- as.formula(paste('HighDollar=="yes"', paste(houseVars,collapse=" + "),sep=" ~ "))
library(rpart)
## Fit decision tree model using rpart library
treemodel <- rpart(houseFormula, train)
pred_decision <- predict(treemodel, newdata=train)
y_pred_decision = ifelse(pred_decision > .75, "yes","no")
y_pred_decision
## Making the Confusion Matrix
cm_decision = table(y_pred_decision,train$HighDollar)
cm_decision
precision(cm_decision)
recall(cm_decision)
## W/ Feature Engineering
## 1 tree, training 0.9203822 0.7096442 0.4059239
accuracyMeasures(predict(treemodel, newdata=test),
test$HighDollar=="yes",
name="tree, test",.75)
pred_decision_test <- predict(treemodel, newdata=test)
y_pred_decision_test = ifelse(pred_decision_test > .75, "yes","no")
cm_decision_test = table(y_pred_decision_test,test$HighDollar)
precision(cm_decision_test)
recall(cm_decision_test)
cm_decision_test
which(test$HighDollar=="no")
count(which(test$HighDollar=="no"))
sum(which(test$HighDollar=="no"))
length(which(test$HighDollar=="no"))
cm_decision_test = table(y_pred_decision_test,test$HighDollar)
cm_decision_test
length(which(test$HighDollar=="yes"))
## Making the Confusion Matrix
cm_bag = table(y_pred_bag,train$HighDollar)
cm_bag
pred_bag <- predict.bag(treelist, newdata=train)
y_pred_bag = ifelse(pred_bag > .75, "yes","no")
y_pred_bag
plot( treelist )
text( tree.carseats , pretty =0)
## Making the Confusion Matrix
cm_bag = table(y_pred_bag,train$HighDollar)
cm_bag
library(caret)
precision(cm_bag)
##[1] 0.9858233
recall(cm_bag)
library(ipred)
pred_bag_test <- predict.bag(treelist, newdata=test)
y_pred_bag_test = ifelse(pred_bag_test > .75, "yes","no")
y_pred_bag_test
cm_bag = table(y_pred_bag_test,test$HighDollar)
cm_bag_test = table(y_pred_bag_test,test$HighDollar)
cm_bag_test
treelist
