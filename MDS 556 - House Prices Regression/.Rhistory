books.ses.paperback.rsme <- round(accuracy(books.ses.paperback),2)
## Paperback RSME = 33.64
books.ses.hardcover.rsme <- round(accuracy(books.ses.hardcover),2)
## Hardcover RSME = 31.93
books.holt.paperback.rsme <- round(accuracy(books.holt.paperback),2)
## Paperback RSME = 31.14
books.holt.hardcover.rsme <- round(accuracy(books.holt.hardcover),2)
## Hardcover RSME = 27.19
books.ses.paperback$mean+1.96*
##2
## The plastics data set consists of the monthly sales (in thousands)
## of product A for a plastics manufacturer for five years.
## a. Plot the time series of sales of product A.
##    Can you identify seasonal fluctuations and/or a trend-cycle?
autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Year") +
ggtitle("Classical multiplicative decomposition
of product A ")
books.ses.paperback$upper[1,'95%']
books.ses.paperback$lower[1,'95%']
books.ses.paperback$mean+1.96* books.ses.paperback.rsme
books.ses.paperback$mean-1.96* books.ses.paperback.rsme
books.ses.paperback$mean + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] + 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
books.ses.paperback$mean[1] - 1.96 * books.ses.paperback.rsme
s <- sqrt(books.holt.paperback$model$mse)
s
View(books.holt.hardcover)
low <- books.holt.paperback$mean[1] - 1.96 * s
books.holt.hardcover
books.holt.paperback
c(low = low, high= high)
high <- books.holt.paperback$mean[1] + 1.96 * s
c(low = low, high= high)
s <- sqrt(books.holt.hardcover$model$mse)
high <- books.holt.hardcover$mean[1] + 1.96 * s
low <- books.holt.hardcover$mean[1] - 1.96 * s
books.holt.hardcover
c(low = low, high= high)
books.holt.paperback.level <- holt(books.original[,'Paperback'], h=4, level =95)
books.holt.paperback.level
books.holt.paperback
books.holt.hardcover
books.holt.paperback
rm(list = ls())
library(expsmooth)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
austa
library(fpp2)
austa
autoplot(austa)
autoplot(usgdp)
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
austa %>% Arima(order=c(0,1,0), include.constant = FALSE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE)
austa %>% Arima(order=c(2,1,3), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa.auto.arima <- austa %>% auto.arima()
austa.auto.arima %>% summary()
## ARIMA(0,1,1) with drift
## AIC = -15.24
## AICc = -14.46
austa.auto.arima %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(2,1,0), include.constant = TRUE)
austa %>% Arima(order=c(2,1,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(2,1,3), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
##      AIC -13.42
##      AICc  -12.09
austa %>% Arima(order=c(0,0,1), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
austa %>% Arima(order=c(0,0,1), include.constant = TRUE)
austa %>% Arima(order=c(0,0,0), include.constant = TRUE) %>% forecast(h=10) %>% autoplot()
## AICc 108.03
## AIC 107.28
austa %>% Arima(order=c(0,0,0), include.constant = TRUE)
austa %>% Arima(order=c(0,2,1), include.constant = FALSE) %>% forecast(h=10) %>% autoplot()
## Chapter 8 Exercises 8 & 9
library(expsmooth)
library(forecast)
library(fpp2)
library(urca)
rm(list = ls())
autoplot(usgdp)
autoplot(BoxCox(usgpd, BoxCox.lambda(usgdp)))
## De-trend series and reduce variances
autoplot(BoxCox(usgdp, BoxCox.lambda(usgdp)))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggAcf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ggPacf()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% sumamry()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% ur.kpss() %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,0), include.constant=FALSE)
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% summary()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(3,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,0))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(1,1,1))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,2))
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% auto.arima()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% checkresiduals()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast() %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>% arima(order=c(2,1,1)) %>% forecast(h=10) %>% autoplot()
BoxCox(usgdp, BoxCox.lambda(usgdp)) %>% diff() %>%  auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
usgdp %>% auto.arima() %>% forecast(h=10) %>% autoplot()
usgdp %>% ets %>% forecast(h=10) %>% autoplot()
load("C:/Users/cusey/Downloads/NatalRiskData.rData")
load("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B/psub.RData")
rm("sdata")
dtrain <- subset(psub, ORIGANDGROUP >= 500)
View(psub)
View(psub)
dtrain <- subset(psub, ORIGRANDGROUP >= 500)
dtest <- subset (psubm ORIGRANDGROUP < 500)
dtest <- subset(psub, ORIGRANDGROUP < 500)
## BUILD THE MODEL
model <- lm(log(PINCP, base=10) ~ AGEP + SEX+ COW + SCHIL, data = dtrain)
## BUILD THE MODEL
model <- lm(log(PINCP, base=10) ~ AGEP + SEX+ COW + SCHL, data = dtrain)
## CREATE COLUMN W/ PREDICTION VALUES AGAINST TEST SET
dtest$predLogPINCP <- predict(model, newdata=dtest)
## CREATE COLUMN W/ PREDICTION VALUES AGAINST TRAIN SET
dtrain$predLogPINCP <- predict(model, newdata=dtrain)
libray("ggplot2")
libray("ggplot")
## PLOT THE TEST DATA, USE THE TEST PREDICTIONS ON THE X AXIS
## AND THE Y AXIS AS THE KNOWN OBSERVATIONS (LOG(PINCP, BASE=10))
## AES() = ASTHETICS
ggplot(data=dtest, aes(x=predLogPINCP, y=log(PINCP,base=10))) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predLogPINCP, y=log(PINCP, base=10)),color="black") +
geom_line(aes(x=log(PINCP, base=10), y=log(PINCP, base=10)), color="blue", linetype=2) +
scale_x_continuous(limits=c(4,5)) +
scale_y_continuous(limits=c(3.5,5.5))
## PLOT THE TEST DATA, USE THE TEST PREDICTIONS ON THE X AXIS
## AND THE Y AXIS AS THE KNOWN OBSERVATIONS (LOG(PINCP, BASE=10))
## AES() = ASTHETICS
library(ggplot2)
ggplot(data=dtest, aes(x=predLogPINCP, y=log(PINCP,base=10))) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predLogPINCP, y=log(PINCP, base=10)),color="black") +
geom_line(aes(x=log(PINCP, base=10), y=log(PINCP, base=10)), color="blue", linetype=2) +
scale_x_continuous(limits=c(4,5)) +
scale_y_continuous(limits=c(3.5,5.5))
## PLOT 2
## PLOT THE RESIDUALS AS A F(X) OF PREDICTED LOG INCOMES
ggplot(data=dtest, aes(x=predLogPINCP, y=predLogPINCP-log(PINCP,base=10))) +
geom_point(alpha=.2, color="black") +
geom_smooth(aes(x=predLogPINCP, y=predLogPINCP-log(PINCP, base=10))color="black")
## PLOT 2
## PLOT THE RESIDUALS AS A F(X) OF PREDICTED LOG INCOMES
ggplot(data=dtest, aes(x=predLogPINCP, y=predLogPINCP-log(PINCP,base=10))) +
geom_point(alpha=.2, color="black") +
geom_smooth(aes(x=predLogPINCP, y=predLogPINCP-log(PINCP, base=10)),color="black")
## Check R-Squared values.
rsq <- function(y,f) {1-sum((y-f)^2)/sum((y-mean(y))^2)}
rsq(log(dtrain$PINCP,base=10), predict(model, newdata=dtrain))
## R-Squared of Test Sest
rsq(log(dtest$PINCP,base=10),predict(model, newdata=dtest))
## RMSE
<- rmse <- function(y,f) {sqrt(mean((y-f)^2))}
## RMSE
rmse <- function(y,f) {sqrt(mean((y-f)^2))}
## RMSE of the Training Data Set
rmse(log(dtrain$PINCP, base=10), predict(model, newdata=dtrain))
##[1] 0.2651856
## RMSE of the Test Data Set
rmse(log(dtest$PINCP, base=10), predict(model,newdata=dtest))
## What is the value of having a bachelor's degree?
## Coefficients measure the value of a independent variable's
## effect on the dependent variable.
coefficients(model)
## ARE THE COEFFICIENTS RELIABLE?
summary(model)
rm()
rm(list = ls())
load("C:/Users/cusey/source/repos/DataScienceCoursework/MDS 556 - 2019 Fall B/NatalRiskData.rData")
## Subset into test/train
train <- sdata[sdata$ORIGRANDGROUP <=5,]
test <- sdata[sdata$ORIGRANDGROUP >5,]
## BUILD MODEL
complications <- c("ULD_MECO", "ULD_PRECIP","ULD_BREECH")
riskfactors <- c("URF_DIAB","URF_CHYPER","URF_PHYPER","URF_ECLAM")
y <- "atRisk"
x <- c("PWGT", "UPREVIS","CIG_REC","GESTREC3","DPLURAL", complications, riskfactors)
## paste() concatencate after converting to character
fmla <- paste(y, paste(x, collapse = "+"), sep="~")
## FIT LOGISTIC REGRESSION MODEL
print(fmla)
model <- glm(fmla, data=train, family=binomial(link="logit"))
## MAKE PREDICTIONS
train$pred <- predict(model, newdata=train, type="response")
test$pred <- predict(model, newdata=test, type="response")
ggplot(train, aes(x=pred, color=atRisk, linetype=atRisk)) +
geom_density()
install.packages(ROCR)
install.packages("ROCR")
##install.packages("ROCR")
library(ROCR)
##install.packages("ROCR")
install.packages("grid")
install.packages("grid")
install.packages("grid")
library(gride)
library(grid)
predObj <- prediction(train$pred,train$atRisk)
##install.packages("ROCR")
##install.packages("grid")
library(ROCR)
install.packages(c("backports", "curl", "digest", "ellipsis", "ggpubr", "prodlim", "R6", "Rcpp", "RcppArmadillo", "rlang", "TTR"))
##install.packages("ROCR")
##install.packages("grid")
library(ROCR)
library(grid)
predObj <- prediction(train$pred,train$atRisk)
predObj <- prediction(train$pred,train$atRisk)
predObj <- prediction(train$pred,train$atRisk)
precObj <- performance(predObj, measure="prec")
recObj <- performance(predObj, measure="rec")
precision <-(precObj@y.values)[[1]]
prec.x <-(precObj@x.values)[[1]]
recall <-(recObj@y.values)[[1]]
rocFrame <- data.frame(threshold=prec.x, precision=precision, recall=recall)
nplot <- function(plist){
n<- length(plist)
grid.newpage()
pushViewport(viewport(layout=grid.layout(n,1)))
vplayout=function(x,y) {viewport(layout.pos.row = x, layout.pos.col = y)}
for(i in 1:n) {
print(plist[[i]], vp=vplayout(i,1))
}
}
pnull <- mean(as.numeric(train$atRisk))
## THIS FUNCTION PLOTS STACKED PLOTS ON A SINGLE PAGE
nplot <- function(plist){
n<- length(plist)
grid.newpage()
pushViewport(viewport(layout=grid.layout(n,1)))
vplayout=function(x,y) {viewport(layout.pos.row = x, layout.pos.col = y)}
for(i in 1:n) {
print(plist[[i]], vp=vplayout(i,1))
}
}
pnull <- mean(as.numeric(train$atRisk))
## PLOT ENRICHMENT RATE AS A F(X) OF THRESHOLD
p1 <-ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim = c(0,10))
library(ggplot2)
## PLOT ENRICHMENT RATE AS A F(X) OF THRESHOLD
p1 <-ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim = c(0,10))
## plot recall as F(X) of Threshold
p2 <- ggplot(rocFrame, aes(x=threshold)) +
geom_line(aes(y=recall)) +
coord_cartesian(xlim=c(0,0.05))
nplot(list(p1,p2))
ctab.test <- table(pred=test$pred > .02, atRisk=test$atRisk)
ctab.test
precision <- ctab.test[2,2]/sum(ctab.test[2,])
precision
recall <-ctab.test[2,2]/sum(ctab.test[,2])
recall
enrich <- precision(mean(as.numeric(test$atRisk)))
enrich <- precision/mean(as.numeric(test$atRisk))
enrich
## FINDING RELATIONS AND EXTRACTING ADVICE:
coefficients(model)
summary(model)
## CALCULATING DEVIANCE RESIDUALS
## CREATE VECTOR OF PREDICTIONS FOR TRAINING DATA
pred <- predict(model, newdata=train, type="response")
llcomponents <- function(y, py) {
y*log(py) + (1-y) * log(1-py)
}
edev <- sign(as.numeric(train$atRisk) - pred *
sqrt(-2*llcomponents(as.numeric(train$atRisk),pred)))
summary(edev)
edev <- sign(as.numeric(train$atRisk) - pred) *
sqrt(-2*llcomponents(as.numeric(train$atRisk),pred))
summary(edev)
## CREATE A FUNCTION TO CALC THE LOG LIKELIHOOK OF A DATASET.
## Y = OUTCOME IN NUMERIC FORM, PY PREDICTED PROBABILITY THAT Y =1.
loglikelihood <- function(y,py) {
sum(y*log(py) + (1-y)*log(1-py))
}
pnull <- mean(as.numeric(train$atRisk))
null.dev <- 2*loglikelihood(as.numeric(train$atRisk),pnull)
pnull
## [1] 0.01920912
null.dev
## [1] -2698.716 why did I get negative here?
model$null.deviance
## [1] 2698.716
pred <- predict(model, newdata=train, type="response")
resid.dev
resid.dev <- 2*loglikelihood(as.numeric(train$atRisk),pred)
resid.dev
## CREATE A FUNCTION TO CALC THE LOG LIKELIHOOK OF A DATASET.
## Y = OUTCOME IN NUMERIC FORM, PY PREDICTED PROBABILITY THAT Y =1.
loglikelihood <- function(y,py) {
sum(y*log(py) + (1-y)*log(1-py))
}
pnull <- mean(as.numeric(train$atRisk))
null.dev <- 2*loglikelihood(as.numeric(train$atRisk),pnull)
pnull
## [1] 0.01920912
null.dev
## [1] -2698.716 why did I get negative here?
model$null.deviance
## [1] 2698.716
pred <- predict(model, newdata=train, type="response")
resid.dev <- 2*loglikelihood(as.numeric(train$atRisk),pred)
resid.dev
model$deviance
pred <- predict(model, newdata=train, type="response")
##[1] 2462.992
testy <- as.numeric(test$atRisk)
testpred<-predict(model,newdata=test,type="response")
pnull.test <-mean(testy)
null.dev.test <- 2*loglikelihood(testy, pnull.test)
null.dev.test <- -2*loglikelihood(testy, pnull.test)
resid.dev.test <- -2*loglikelihood(testy,testpred)
pnull.test
## [1] 0.0172713
null.dev.test
## [1] 2110.91
resid.dev.test
## comment this code after I'm satisfied with
## feature engineering portion
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
sapply(data,function(x) sum(is.na(x)))
## Correlation Coefficients #####################
##library(help = "stats")
plot(data$LotArea, data$SalePrice, col="red", xlab="Year Built", ylab="Sales Price", main="Pearson Correlation")
rfe_controller <- rfeControl(functions = lmFuncs, method="repeatedcv", repeats = 5, verbose = FALSE)
library(caret)
## comment this code after I'm satisfied with
## feature engineering portion
setwd('C:/Users/cusey/source/repos/DataScienceProjects/MDS 556 - House Prices Regression')
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
sapply(data,function(x) sum(is.na(x)))
##install.packages("FSelector")
library(FSelector)
##Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_231') # for 32-bit
ig_values <-information.gain(SalePrice~.,data)
ig_values
top_k_features <- cutoff.k(ig_values,2)
f<- as.simple.formula(top_k_features,"SalePrice")
f
library(caret)
rfe_controller <- rfeControl(functions = lmFuncs, method="repeatedcv", repeats = 5, verbose = FALSE)
size <- c(1:10)
##names(reg.data) Help me find indices
data("mtcars")
lm_Profiler <- rfe(x=data[,1:12], y=data[,13], sizes=size,rfeControl = rfe_controller)
lm_Profiler
## Feature Extraction or Construction
range(data$LotArea)
scale(data$LotArea)
## Dimensionality Reduction
##PCA
## doesn't like my factors? Need to figure
## out how to scale them.
sale_price_features <- data[3:4]
sale_price_target <- data[13]
sale_price_pca <- prcomp(x=sale_price_features, scale. = T)
biplot(sale_price_pca,scale = TRUE, pc.biplot = TRUE)
lda_model <- lda(SalesPrice~., data=data, subset=data)
lda_model$means
library(MASS)
lda_model <- lda(SalesPrice~., data=data, subset=data)
lda_model$means
lda_model <- lda(SalePrice~., data=data, subset=data)
lda_model$means
library(MASS)
lda_model <- lda(SalePrice~., data=data, subset=data)
## Correlation Coefficients #####################
##library(help = "stats")
plot(data$LotArea, data$SalePrice, col="red", xlab="Year Built", ylab="Sales Price", main="Pearson Correlation")
cor(x=data$LotArea, y = data$SalePrice, use="everything", method="pearson")
## data preprocessing
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
View(data)
View(data)
data[3,4]
data[,c(3,4)]
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
data <- data[,c(3,4,13)]
data <- dataPreprocessing()
library(caTools)
set.seed(100)
set.seed(100)
split <-sample.split(data$SalePrice, SplitRatio=.8)
train = subset(data, split == TRUE)
test = subset(data, split==FALSE)
rm("split")
source("data preprocessing.R", local = TRUE)
data <- dataPreprocessing()
test <- splitData("train",.8,data)
test <- splitData("test",.8,data)
test <- splitData("train",.8,data)
test <- splitData("test",.8,data)
train <- splitData("train",.8,data)
test <- splitData("test",.8,data)
## FITTING MULTIPLE LINEAR REGRESSION TO THE TRAINING SET
regressor = lm(formula = SalePrice ~ .,
data = train)
summary(regressor)
summary(regressor)
## CREATE COLUMN W/ PREDICTION VALUES AGAINST TEST SET
test$predSalePrice <- predict(regressor, newdata=test)
## CREATE COLUMN W/ PREDICTION VALUES AGAINST TRAIN SET
train$predSalePrice <- predict(regressor, newdata=train)
library(ggplot2)
library(ggplot2)
ggplot(data=test, aes(x=predSalePRice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=log(SalePrice, base=10)),color="black") +
geom_line(aes(x=log(predSalePrice, base=10), y=SalePrice, color="blue", linetype=2) +
}
ggplot(data=test, aes(x=predSalePRice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=log(SalePrice, base=10)),color="black") +
geom_line(aes(x=predSalePrice, y=SalePrice, color="blue", linetype=2))
ggplot(data=test, aes(x=predSalePRice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=SalePrice, base=10),color="black") +
geom_line(aes(x=predSalePrice, y=SalePrice, color="blue", linetype=2))
ggplot(data=test, aes(x=predSalePRice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=SalePrice,color="black") +
geom_line(aes(x=predSalePrice, y=SalePrice, color="blue", linetype=2))
}
ggplot(data=test, aes(x=predSalePRice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=SalePrice,color="black")) +
geom_line(aes(x=predSalePrice, y=SalePrice, color="blue", linetype=2))
ggplot(data=test, aes(x=predSalePrice, y=SalePrice)) +
geom_point(alpha = .2, color="black") +
geom_smooth(aes(x=predSalePrice, y=SalePrice,color="black")) +
geom_line(aes(x=predSalePrice, y=SalePrice, color="blue", linetype=2))
ggplot(data=test, aes(x=predSalePrice, y=SalePrice)) ##+
library(ggplot2)
ggplot(data=test, aes(x=predSalePrice, y=SalePrice)) ##+
ggplot(data=test, x=predSalePrice, y=SalePrice) ##+
top_k_features <- cutoff.k(ig_values,2)
f<- as.simple.formula(top_k_features,"SalePrice")
##install.packages("FSelector")
library(FSelector)
##Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_231') # for 32-bit
ig_values <-information.gain(SalePrice~.,data)
ig_values
top_k_features <- cutoff.k(ig_values,2)
f<- as.simple.formula(top_k_features,"SalePrice")
f
library(caret)
rfe_controller <- rfeControl(functions = lmFuncs, method="repeatedcv", repeats = 5, verbose = FALSE)
size <- c(1:10)
size <- c(1:12)
##names(reg.data) Help me find indices
data("mtcars")
lm_Profiler <- rfe(x=data[,1:12], y=data[,13], sizes=size,rfeControl = rfe_controller)
lm_Profiler
top_k_features <- cutoff.k(ig_values,2)
f<- as.simple.formula(top_k_features,"SalePrice")
f
force(mtcars)
## FITTING MULTIPLE LINEAR REGRESSION TO THE TRAINING SET
regressor = lm(formula = f,
data = train)
summary(regressor)
## SalePrice ~ Neighborhood + MSSubClass
## rerun with less dimensions
## FITTING MULTIPLE LINEAR REGRESSION TO THE TRAINING SET
regressor2 = lm(formula = f ~ .,
data = train)
summary(regressor2)
f<- as.simple.formula(top_k_features,"SalePrice")
f
## SalePrice ~ Neighborhood + MSSubClass
## rerun with less dimensions
## FITTING MULTIPLE LINEAR REGRESSION TO THE TRAINING SET
regressor2 = lm(formula = f ~ .,
data = train)
## SalePrice ~ Neighborhood + MSSubClass
## rerun with less dimensions
## FITTING MULTIPLE LINEAR REGRESSION TO THE TRAINING SET
regressor2 = lm(formula = f,
data = train)
summary(regressor2)
