{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook reproduces code from Feature Engineering for Machine Learning by Alic Zheng and Amanda Casari.\n",
    "\n",
    "In parallel, the same concepts are applied to a Twitter dataset that seeks to identify tweets suggesting a true natural disaster is occurring. See https://www.kaggle.com/c/nlp-getting-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "n-grams are sequences of n tokens. 1-grams (unigram) are just the frequency count of distinct words. 2-grams are unique 2 word pairings. While the code is taken from the book, I consolidated the code into functions for reuseability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11)\n",
      "(7613, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import spacy\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as modsel\n",
    "import sklearn.preprocessing as preproc\n",
    "\n",
    "# Online News Popularity Data Set - first 10,000\n",
    "yelp_df = pd.read_csv('C://Users/Megan.Cusey/Documents/GitHub/DataScienceProjects/MDS 564 - Twitter NLP Text Analysis/Yelp Reviews - 10000.csv', nrows=10000)\n",
    "print(yelp_df.shape)\n",
    "\n",
    "twitter_df = pd.read_csv('C://Users/Megan.Cusey/Documents/GitHub/DataScienceProjects/MDS 564 - Twitter NLP Text Analysis/twitter_train.csv')\n",
    "print(twitter_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_and_ngrams(df, text_column):\n",
    "    # Creat feature transformations for unigrams, bigrams, and trigrams.\n",
    "    # Default ignored single character words, but this examples explicitely includes them.\n",
    "    bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "    bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "    trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "    # Fit transformers and look at vocab size\n",
    "    bow_converter.fit(df[text_column])\n",
    "    words = bow_converter.get_feature_names()\n",
    "    bigram_converter.fit(df[text_column])\n",
    "    bigrams = bigram_converter.get_feature_names()\n",
    "    trigram_converter.fit(df[text_column])\n",
    "    trigrams = trigram_converter.get_feature_names()\n",
    "\n",
    "    print(\"Lengths of BOW, Bigrams, and Trigrams:\",\n",
    "           \"\\n Words:\",len(words),\n",
    "           \"\\n Bigrams:\", len(bigrams),\n",
    "           \"\\n Trigrams:\", len(trigrams),\n",
    "           \"\\n\\n\")\n",
    "    \n",
    "    return words, bigrams, trigrams\n",
    "\n",
    "\n",
    "def view_results(words, bigrams, trigrams):\n",
    "    print(\"Sample of Words: \\n\", words[:10],\"\\n\\n\",\n",
    "          \"Sample of Bigrams: \\n\", bigrams[-10:],\"\\n\\n\",\n",
    "          \"Sample of Trigrams: \\n\", trigrams[:10],\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of BOW, Bigrams, and Trigrams: \n",
      " Words: 29221 \n",
      " Bigrams: 368937 \n",
      " Trigrams: 881609 \n",
      "\n",
      "\n",
      "Sample of Words: \n",
      " ['0', '00', '000', '007', '00a', '00am', '00pm', '01', '02', '03'] \n",
      "\n",
      " Sample of Bigrams: \n",
      " ['zuzu was', 'zuzus room', 'zweigel wine', 'zwiebel kräuter', 'zy world', 'zzed in', 'éclairs napoleons', 'école lenôtre', 'ém all', 'òc châm'] \n",
      "\n",
      " Sample of Trigrams: \n",
      " ['0 0 eye', '0 20 less', '0 39 oz', '0 39 pizza', '0 5 i', '0 50 to', '0 6 can', '0 75 oysters', '0 75 that', '0 75 to'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yelp Words, Bigrams, and Trigrams\n",
    "words, bigrams, trigrams = bow_and_ngrams(yelp_df,\"text\")\n",
    "\n",
    "view_results(words, bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of BOW, Bigrams, and Trigrams: \n",
      " Words: 21678 \n",
      " Bigrams: 69982 \n",
      " Trigrams: 87447 \n",
      "\n",
      "\n",
      "Sample of Words: \n",
      " ['0', '00', '000', '0000', '007npen6lg', '00cy9vxeff', '00end', '00pm', '01', '02'] \n",
      "\n",
      " Sample of Bigrams: \n",
      " ['ûó oh', 'ûó organizers', 'ûó rt', 'ûó the', 'ûó wallybaiter', 'ûóher upper', 'ûókody vine', 'ûónegligence and', 'ûótech business', 'ûówe work'] \n",
      "\n",
      " Sample of Trigrams: \n",
      " ['0 11 ronnie', '0 45 to', '0 6 8km', '0 75 in', '0 9 northern', '0 amp more', '0 and blew', '0 balls 0', '0 bids û_', '0 but dude'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Twitter NLP Words, Bigrams, and Trigrams\n",
    "words, bigrams, trigrams = bow_and_ngrams(twitter_df,\"text\")\n",
    "\n",
    "view_results(words, bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Part of Speech Tagging\n",
    "Chunking forms sequences of words (tokens) based off of parts of speech.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_chunking(df, text_column):\n",
    "    ## Preload English Language\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    ## Create pandas dataframe of spaCy nlp variables\n",
    "    doc_df = df[text_column].apply(nlp)\n",
    "    \n",
    "    for doc in doc_df[4]:\n",
    "        print([doc.text, doc.pos_, doc.tag_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General', 'PROPN', 'NNP']\n",
      "['Manager', 'PROPN', 'NNP']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['Petello', 'PROPN', 'NNP']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['a', 'DET', 'DT']\n",
      "['good', 'ADJ', 'JJ']\n",
      "['egg', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['Not', 'ADV', 'RB']\n",
      "['to', 'PART', 'TO']\n",
      "['go', 'VERB', 'VB']\n",
      "['into', 'ADP', 'IN']\n",
      "['detail', 'NOUN', 'NN']\n",
      "[',', 'PUNCT', ',']\n",
      "['but', 'CCONJ', 'CC']\n",
      "['let', 'VERB', 'VB']\n",
      "['me', 'PRON', 'PRP']\n",
      "['assure', 'VERB', 'VB']\n",
      "['you', 'PRON', 'PRP']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['have', 'VERB', 'VBP']\n",
      "['any', 'DET', 'DT']\n",
      "['issues', 'NOUN', 'NNS']\n",
      "['(', 'PUNCT', '-LRB-']\n",
      "['albeit', 'ADP', 'IN']\n",
      "['rare', 'ADJ', 'JJ']\n",
      "[')', 'PUNCT', '-RRB-']\n",
      "['speak', 'VERB', 'VBP']\n",
      "['with', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['treat', 'VERB', 'VB']\n",
      "['the', 'DET', 'DT']\n",
      "['guy', 'NOUN', 'NN']\n",
      "['with', 'ADP', 'IN']\n",
      "['some', 'DET', 'DT']\n",
      "['respect', 'NOUN', 'NN']\n",
      "['as', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['state', 'VERB', 'VBP']\n",
      "['your', 'ADJ', 'PRP$']\n",
      "['case', 'NOUN', 'NN']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['I', 'PRON', 'PRP']\n",
      "[\"'d\", 'VERB', 'MD']\n",
      "['be', 'VERB', 'VB']\n",
      "['surprised', 'ADJ', 'JJ']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['do', 'VERB', 'VBP']\n",
      "[\"n't\", 'ADV', 'RB']\n",
      "['walk', 'VERB', 'VB']\n",
      "['out', 'ADV', 'RB']\n",
      "['totally', 'ADV', 'RB']\n",
      "['satisfied', 'ADJ', 'JJ']\n",
      "['as', 'ADP', 'IN']\n",
      "['I', 'PRON', 'PRP']\n",
      "['just', 'ADV', 'RB']\n",
      "['did', 'VERB', 'VBD']\n",
      "['.', 'PUNCT', '.']\n",
      "['Like', 'INTJ', 'UH']\n",
      "['I', 'PRON', 'PRP']\n",
      "['always', 'ADV', 'RB']\n",
      "['say', 'VERB', 'VBP']\n",
      "['.....', 'PUNCT', 'NFP']\n",
      "['\"', 'PUNCT', '``']\n",
      "['Mistakes', 'NOUN', 'NNS']\n",
      "['are', 'VERB', 'VBP']\n",
      "['inevitable', 'ADJ', 'JJ']\n",
      "[',', 'PUNCT', ',']\n",
      "['it', 'PRON', 'PRP']\n",
      "[\"'s\", 'VERB', 'VBZ']\n",
      "['how', 'ADV', 'WRB']\n",
      "['we', 'PRON', 'PRP']\n",
      "['recover', 'VERB', 'VBP']\n",
      "['from', 'ADP', 'IN']\n",
      "['them', 'PRON', 'PRP']\n",
      "['that', 'ADJ', 'WDT']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['important', 'ADJ', 'JJ']\n",
      "['\"', 'PUNCT', \"''\"]\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['\\n\\n', 'SPACE', '_SP']\n",
      "['Thanks', 'NOUN', 'NNS']\n",
      "['to', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['his', 'ADJ', 'PRP$']\n",
      "['awesome', 'ADJ', 'JJ']\n",
      "['staff', 'NOUN', 'NN']\n",
      "['.', 'PUNCT', '.']\n",
      "['You', 'PRON', 'PRP']\n",
      "[\"'ve\", 'VERB', 'VB']\n",
      "['got', 'VERB', 'VBN']\n",
      "['a', 'DET', 'DT']\n",
      "['customer', 'NOUN', 'NN']\n",
      "['for', 'ADP', 'IN']\n",
      "['life', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['..........', 'PUNCT', 'NFP']\n",
      "[':', 'PUNCT', ':']\n",
      "['^', 'PUNCT', 'NFP']\n",
      "[')', 'PUNCT', '-RRB-']\n"
     ]
    }
   ],
   "source": [
    "## Yelp Chunking & POS Tagging\n",
    "english_chunking(yelp_df,\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'ADV', 'RB']\n",
      "['got', 'VERB', 'VBD']\n",
      "['sent', 'VERB', 'VBN']\n",
      "['this', 'DET', 'DT']\n",
      "['photo', 'NOUN', 'NN']\n",
      "['from', 'ADP', 'IN']\n",
      "['Ruby', 'PROPN', 'NNP']\n",
      "['#', 'PROPN', 'NNP']\n",
      "['Alaska', 'PROPN', 'NNP']\n",
      "['as', 'ADP', 'IN']\n",
      "['smoke', 'NOUN', 'NN']\n",
      "['from', 'ADP', 'IN']\n",
      "['#', 'NOUN', 'NN']\n",
      "['wildfires', 'NOUN', 'NNS']\n",
      "['pours', 'NOUN', 'NNS']\n",
      "['into', 'ADP', 'IN']\n",
      "['a', 'DET', 'DT']\n",
      "['school', 'NOUN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "## Twitter Chunking & POS Tagging\n",
    "english_chunking(twitter_df,\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3272\n",
      "4343\n",
      "(6090, 5)\n",
      "(1523, 5)\n"
     ]
    }
   ],
   "source": [
    "# The book used Yelp Academic Data Set which I couldn't find posted. It's different then the 10,000 review CSV data we have.\n",
    "# In the text, it discusses an imbalance for two categories that they are attempting to use as the target for a classification \n",
    "# problem. The text follows the following steps to address the issue:\n",
    "# 1. They identify the two categories\n",
    "# 2. Subset each of them into new data frames\n",
    "# 3. Randomly select an even number of observations from each subset\n",
    "# 4. Combine the randomly selected observations into a single data frame.\n",
    "# 5. Compute test/train datasets that have a balanced amount of observations from both categories.\n",
    "\n",
    "# The # of records aren't that imbalanced here so I dont' think it's necessary to do. It doesn't make me wonder what\n",
    "# the difference would be if we DID balance the TRUE and FALSE observations... perhaps I'll try that.\n",
    "\n",
    "# In addition, it makes me wonder about the Caravan dataset we discussed the first week on the course where only 5% of\n",
    "# the observations we were trying to identify existed in the data set... perhaps I'll revisit it as well.\n",
    "\n",
    "print(len(twitter_df.loc[twitter_df['target'] == 1])+1)\n",
    "print(len(twitter_df.loc[twitter_df['target'] == 0])+1)\n",
    "\n",
    "# Split to train/test\n",
    "train_df, test_df = modsel.train_test_split(twitter_df, train_size=.8, random_state = 123)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 18558)\n",
      "(1523, 7081)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 7081 features per sample; expecting 18558",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-ae6bece14052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_bow_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_bow_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 273\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 7081 features per sample; expecting 18558"
     ]
    }
   ],
   "source": [
    "# Compare BOW, l2 normalization, and TF-IDF for linear classification\n",
    "\n",
    "## BOW\n",
    "bow_transform = text.CountVectorizer()\n",
    "x_bow_train = bow_transform.fit_transform(train_df['text'])\n",
    "x_bow_test = bow_transform.fit_transform(test_df['text'])\n",
    "print(x_bow_train.shape)\n",
    "print(x_bow_test.shape)\n",
    "len(bow_transform.vocabulary_)\n",
    "\n",
    "## TF-IDF\n",
    "tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "x_tfidf_train = tfidf_transform.fit_transform(x_bow_train)\n",
    "x_tfidf_test = tfidf_transform.fit_transform(x_bow_test)\n",
    "\n",
    "## L2\n",
    "x_l2_train = preproc.normalize(x_bow_train, axis = 0)\n",
    "x_l2_test = preproc.normalize(x_bow_test, axis = 0)\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_test = test_df['target']\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "m = SGDClassifier().fit(x_bow_train, y_train)\n",
    "s = m.score(x_bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megan.Cusey\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 7081 features per sample; expecting 18558",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-e06af13b8a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_logistic_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_bow_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_bow_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BOW'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_logistic_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tfidf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tfidf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TF-IDF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mm3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_logistic_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_l2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'L2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-e06af13b8a8f>\u001b[0m in \u001b[0;36msimple_logistic_classify\u001b[1;34m(X_tr, y_tr, X_test, y_test, description, _C)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m## Helper function to train a logistic classifier and score on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Test score with'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'features:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\development\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 273\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 7081 features per sample; expecting 18558"
     ]
    }
   ],
   "source": [
    "def simple_logistic_classifier(x_train, y_train, x_test, y_test, description):\n",
    "    m=LogisticRegression().fit(x_train,y_train)\n",
    "    s=m.score(x_test,y_test)\n",
    "    print('Test score with ',description,' features: ', s)\n",
    "    return m\n",
    "\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    ## Helper function to train a logistic classifier and score on test data\n",
    "    m = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print ('Test score with', description, 'features:', s)\n",
    "    return m\n",
    "\n",
    "m1 = simple_logistic_classify(x_bow_train, y_train, x_bow_test, y_test, 'BOW')\n",
    "m2 = simple_logistic_classifier(x_tfidf_train, y_train, x_tfidf_test, y_test, 'TF-IDF')\n",
    "m3 = simple_logistic_classifier(x_l2_train, y_train, x_l2_test, y_test, 'L2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
