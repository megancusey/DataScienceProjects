{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Helpful Links: [Kaggle: Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview), [Getting Started with NLP](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro), [All you need to know about Text Preprocessing for Machine Learning & NLP](https://kavita-ganesan.com/text-preprocessing-tutorial/#.XkMU3Gi6OUl), [deepai.org](https://deepai.org/machine-learning-glossary-and-terms/f-score), [Beyond Accuracy: Precision and Recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c), [Kaggle NLP Course](https://www.kaggle.com/learn/natural-language-processing?utm_medium=email&utm_source=intercom&utm_campaign=nlp-course-launch), [Simple Implementation Of Word2Vec](https://www.kaggle.com/slatawa/simple-implementation-of-word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See my [GitHub Repo](https://github.com/megancusey/DataScienceProjects/tree/master/MDS%20564%20-%20Twitter%20NLP%20Text%20Analysis) for all files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundation of this use case is to evaluate the ability to use tweets from Twitter users around the world to identify whether the tweet indicates a disasterous situation. The underlying questions this project seeks to answser is: When is a disaster occurring in real time? The ability to identify tweets implying a real time disaster with an acceptable degree of accuracy would enable emergency response teams to act quicker and news media outlets to report on the issue in a more timely manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets can be posted 24/7 by millions of users. Per [David Sayce](https://www.dsayce.com/social-media/tweets-day/), a digital consultant, in October 2019 there was an average of 350,000 tweets per minute. There are numerous concerns when it comes to text analysis and the application on data that is volumnous, poor structure, different languages, and in many parts of the world.\n",
    "\n",
    "Unfortunately, I was unable to discover answers to the following questions:\n",
    "\n",
    "1.) What was the criteria when pulling in the train/test set. It appears to be all in English so some subset must have occurred. We also don't have an idea about WHEN the tweets occurred which could have some relevant value.\n",
    "\n",
    "2.) There are some duplicate tweets with different labels. This is a mistake in the train/test set or perhaps they represent retweets, but retweeting doesn't change the context of the tweet and if it's related to a disaster. Why the different labels?\n",
    "\n",
    "3.) What constitutes a disaster? There are natural disasters, political disasters. Some are subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "Read in both the test and train set for the twitter data. \n",
    "\n",
    "Note that the train set is composed of 5 columns (one being the target column) and 7613 observations. The test set is made of 4 columns (excludes the target column) and has 3263 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system management\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read in training data\n",
    "#train_df = pd.read_csv('twitter_train.csv') # original training set\n",
    "train_df = pd.read_csv('clean_training_data_0641p.csv') ## load this in order to skip the data pre-processing computation time.\n",
    "\n",
    "\n",
    "# Read in test data\n",
    "#test_df = pd.read_csv('twitter_test.csv')\n",
    "test_df= pd.read_csv('clean_testing_data_1244p.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7613, 6)\n",
      "Test data shape: (3263, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>r people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1         deed reason earthquake may allah forgive u  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  resident asked shelter place notified officer ...  \n",
       "3       1  r people receive wildfire evacuation order cal...  \n",
       "4       1  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training data shape:', train_df.shape)\n",
    "print('Test data shape:', test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Missing Values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "keyword         61\n",
       "location      2533\n",
       "text             0\n",
       "target           0\n",
       "text_clean       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify missing values\n",
    "print(\"Count Missing Values: \")\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17fa40602e8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(train_df['target'].value_counts().index,train_df['target'].value_counts(),palette='rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of label classification when the tweet actually contains the word \"disaster\"\n",
      "1    102\n",
      "0     40\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Examples of tweets that contain the word \"disaster\" but are labeled as non-disasters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>3998</td>\n",
       "      <td>disaster</td>\n",
       "      <td>Los Angeles, London, Kent</td>\n",
       "      <td>I forgot to bring chocolate with me. Major dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>forgot bring chocolate major disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>4003</td>\n",
       "      <td>disaster</td>\n",
       "      <td>Portoviejo-Manabi-Ecuador</td>\n",
       "      <td>I'm a disaster?? https://t.co/VCV73BUaCZ</td>\n",
       "      <td>0</td>\n",
       "      <td>disaster url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>4005</td>\n",
       "      <td>disaster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@LovelyLikeLaura I can see why one of your fav...</td>\n",
       "      <td>0</td>\n",
       "      <td>lovelylikelaura see one favorite book beautifu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>4008</td>\n",
       "      <td>disaster</td>\n",
       "      <td>chillin at ceder rapids</td>\n",
       "      <td>Beautiful disaster // Jon McLaughlin is such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful disaster r jon mclaughlin good song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>4012</td>\n",
       "      <td>disaster</td>\n",
       "      <td>en el pais de los arrechos</td>\n",
       "      <td>beautiful disaster https://t.co/qm5Sz0fyU8</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful disaster url</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   keyword                    location  \\\n",
       "2781  3998  disaster   Los Angeles, London, Kent   \n",
       "2783  4003  disaster   Portoviejo-Manabi-Ecuador   \n",
       "2784  4005  disaster                         NaN   \n",
       "2786  4008  disaster     chillin at ceder rapids   \n",
       "2789  4012  disaster  en el pais de los arrechos   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2781  I forgot to bring chocolate with me. Major dis...       0   \n",
       "2783           I'm a disaster?? https://t.co/VCV73BUaCZ       0   \n",
       "2784  @LovelyLikeLaura I can see why one of your fav...       0   \n",
       "2786  Beautiful disaster // Jon McLaughlin is such a...       0   \n",
       "2789         beautiful disaster https://t.co/qm5Sz0fyU8       0   \n",
       "\n",
       "                                             text_clean  \n",
       "2781              forgot bring chocolate major disaster  \n",
       "2783                                       disaster url  \n",
       "2784  lovelylikelaura see one favorite book beautifu...  \n",
       "2786      beautiful disaster r jon mclaughlin good song  \n",
       "2789                             beautiful disaster url  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Counts of label classification when the tweet actually contains the word \"disaster\"')\n",
    "print(train_df.loc[train_df['text'].str.contains('disaster',na=False, case=False)].target.value_counts())\n",
    "\n",
    "print('\\nExamples of tweets that contain the word \"disaster\" but are labeled as non-disasters')\n",
    "train_df.loc[(train_df['text'].str.contains('disaster',na=False, case=False)) & (train_df['target'] == 0)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Explore the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disaster Tweet:  Forest fire near La Ronge Sask. Canada\n",
      "Not a Disaster Tweet:  I love fruits\n"
     ]
    }
   ],
   "source": [
    "# Example of a disaster tweet\n",
    "disaster_tweets = train_df[train_df['target']==1]['text']\n",
    "print(\"Disaster Tweet: \", disaster_tweets.values[1])\n",
    "\n",
    "# Example of a disaster tweet\n",
    "not_disaster_tweets = train_df[train_df['target']==0]['text']\n",
    "print(\"Not a Disaster Tweet: \",not_disaster_tweets.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Explore Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17fa22f8dd8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(y=train_df['keyword'].value_counts()[:20].index,x=train_df['keyword'].value_counts()[:20], orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Explore location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17fa20d7080>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replacing the ambigious locations name with Standard names\n",
    "train_df['location'].replace({'United States':'USA',\n",
    "                           'New York':'USA',\n",
    "                            \"London\":'UK',\n",
    "                            \"Los Angeles, CA\":'USA',\n",
    "                            \"Washington, D.C.\":'USA',\n",
    "                            \"California\":'USA',\n",
    "                             \"Chicago, IL\":'USA',\n",
    "                             \"Chicago\":'USA',\n",
    "                            \"New York, NY\":'USA',\n",
    "                            \"California, USA\":'USA',\n",
    "                            \"FLorida\":'USA',\n",
    "                            \"Nigeria\":'Africa',\n",
    "                            \"Kenya\":'Africa',\n",
    "                            \"Everywhere\":'Worldwide',\n",
    "                            \"San Francisco\":'USA',\n",
    "                            \"Florida\":'USA',\n",
    "                            \"United Kingdom\":'UK',\n",
    "                            \"Los Angeles\":'USA',\n",
    "                            \"Toronto\":'Canada',\n",
    "                            \"San Francisco, CA\":'USA',\n",
    "                            \"NYC\":'USA',\n",
    "                            \"Seattle\":'USA',\n",
    "                            \"Earth\":'Worldwide',\n",
    "                            \"Ireland\":'UK',\n",
    "                            \"London, England\":'UK',\n",
    "                            \"New York City\":'USA',\n",
    "                            \"Texas\":'USA',\n",
    "                            \"London, UK\":'UK',\n",
    "                            \"Atlanta, GA\":'USA',\n",
    "                            \"Mumbai\":\"India\"},inplace=True)\n",
    "\n",
    "sns.barplot(y=train_df['location'].value_counts()[:5].index,x=train_df['location'].value_counts()[:5],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#once preprocessing steps have been identified and complete, combine into one function such as below:\n",
    "\n",
    "# Convert text to lower and spell out contractions\n",
    "def general_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"i'll\", \"i will\", text)\n",
    "    text = re.sub(r\"she'll\", \"she will\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"here's\", \"here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "    text = text.split()\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text\n",
    "             if word not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    " # Repalce URLS \n",
    "def replace_URLS(text):\n",
    "    url= re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'URL',text)\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "# Remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# correct mispelled words\n",
    "def correct_mispelled_words(text):\n",
    "    correct_text=[]\n",
    "    incorrect_text = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in incorrect_text:\n",
    "            correct_text.append(spell.correction(word))\n",
    "        else:\n",
    "            correct_text.append(word)\n",
    "    return \" \".join(correct_text)\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "#Combine Text\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "# Execute text preprocessing functions\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    text = replace_URLS(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = correct_mispelled_words(text)\n",
    "    text = general_preprocessing(text)\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = combine_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Save to CSV so that we can import the CSV files after text preprocessing\n",
    "def generate_CSV(df,file_name):\n",
    "    file_name = file_name+\".csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# This function decides if text preprocessing has occurred or not\n",
    "# IE: if the CSV file we created during a previous text preprocessing\n",
    "# has been loaded or if text preprocessing steps need to be complete.\n",
    "def _text_preprocessing(df,df_type):\n",
    "    if \"text_clean\" not in df:\n",
    "        # If for some reason, text was NULL, remove it\n",
    "        df = df.dropna(subset=[\"text\"])\n",
    "        \n",
    "        # Assign the text_clean column to all values of text\n",
    "        df[\"text_clean\"] = df[\"text\"]\n",
    "        \n",
    "        # Record computation time\n",
    "        start= timeit.default_timer()\n",
    "        \n",
    "        #Loop over all rows of the dataframe\n",
    "        for r in range(0, len(df)):\n",
    "            \n",
    "            # This allows the continuous output for the execution time.\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            #Execute Text Preprocessing steps\n",
    "            df['text_clean'].values[r] = text_preprocessing(df['text'].values[r])\n",
    "                \n",
    "            # Stop timer since steps have been completed\n",
    "            stop = timeit.default_timer()\n",
    "    \n",
    "            # Calculate when the tasks should be complete.\n",
    "            if (r/len(df)*100<5) :\n",
    "                expected_time = \"Calculating...\"\n",
    "            else:\n",
    "                time_perc = timeit.default_timer()\n",
    "                expected_time = np.round((time_perc-start)/(r/len(df))/60,2)\n",
    "            \n",
    "            # Advises if we are on the test file or train file\n",
    "            print(df_type)\n",
    "            print(\"Current Progress: \", np.round(r/len(df)*100,2),\"%\")\n",
    "            print(\"Current Run Time: \", np.round((stop-start)/60,2),\" minutes\")\n",
    "            print(\"Expected Run Time: \", expected_time, \" minutes\")\n",
    "            print(\"\\n\", df['text_clean'].values[r])\n",
    "\n",
    "        #If after the text preprocessing steps, if the text is now empty, remove it/\n",
    "        df = df.dropna(subset=[\"text_clean\"])\n",
    "        # save to file so we can just load this next time.\n",
    "        filename = \"clean_{1}_{0:%I%Mp}\".format(datetime.datetime.now(),df_type)\n",
    "        generate_CSV(df,filename)\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_df = _text_preprocessing(train_df,\"training_data\")\n",
    "test_df = _text_preprocessing(test_df,\"testing_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "# I'd like to eventually convert these preprocessing steps into a pipeline\n",
    "#from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#class Lemmatize_Text(BaseEstimator, TransformerMixin):\n",
    "#    def __init__(self, lemmatize=True):\n",
    "#        self.lemmatize=lemmatize\n",
    "#    def fit(self,X,y=non):\n",
    "#        return self\n",
    "#    def transform(self, X, y=none):\n",
    "#        if self.lemmatize:\n",
    "#            from nltk.stem import WordNetLemmatizer\n",
    "#            lem = WordNetLemmatizer()\n",
    "#            text_clean = [lem.lemmatize(word) for word in text_clean]\n",
    "#            text_clean = ' '.join(text)\n",
    "#            \n",
    "#            return np.c__[X, textt_clean]\n",
    "#        else\n",
    "#            return np.c__[X]\n",
    "\n",
    "test_df = test_df.dropna(subset=[\"text_clean\"])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>r people receive wildfire evacuation order cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target                                         text_clean\n",
       "0   1       1         deed reason earthquake may allah forgive u\n",
       "1   4       1              forest fire near la ronge sask canada\n",
       "2   5       1  resident asked shelter place notified officer ...\n",
       "3   6       1  r people receive wildfire evacuation order cal...\n",
       "4   7       1  got sent photo ruby alaska smoke wildfire pour..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop location, keyword, and text. Location and Keyword has too many null values. We did the preprocessing off of text\n",
    "# and no longer need that column.\n",
    "train_df = train_df.drop(\"location\",axis=1)\n",
    "train_df = train_df.drop(\"keyword\",axis=1)\n",
    "train_df = train_df.drop(\"text\",axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         text_clean\n",
       "0   0                        happened terrible car crash\n",
       "1   2  heard earthquake different cities stay safe ev...\n",
       "2   3  forest fire spot pond geese fleeing across str...\n",
       "3   9               apocalypse lighting spokane wildfire\n",
       "4  11             typhoon soudelor kills 28 china taiwan"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop location, keyword, and text. Location and Keyword has too many null values. We did the preprocessing off of text\n",
    "# and no longer need that column.\n",
    "test_df = test_df.drop(\"location\",axis=1)\n",
    "test_df = test_df.drop(\"keyword\",axis=1)\n",
    "test_df = test_df.drop(\"text\",axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "## BOW\n",
    "bow_transform = CountVectorizer()\n",
    "bow_train = bow_transform.fit_transform(train_df['text_clean'])\n",
    "bow_test = bow_transform.fit_transform(test_df['text_clean'])\n",
    "\n",
    "print(bow_train[0].todense()) #why? revisit page 64 of hands on ml book\n",
    "\n",
    "## TFIDF\n",
    "tfidf_transform = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "tfidf_train = tfidf_transform.fit_transform(train_df['text_clean'])\n",
    "tfidf_test = tfidf_transform.fit_transform(test_df['text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "cross_val_score = only cross validation\n",
    "\n",
    "GridSearchCV = cross validation and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Average score was lower than I expect. I think the logistic regression underfits the data or perhpas more preprocessing of the text could enable us to use a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.60861423 0.54961832 0.6192     0.54445463 0.70202808]\n",
      "\n",
      " AVERAGE OF SCORES:  0.6047830525449209\n",
      "\n",
      " STANDARD DEVIATION:  0.0572045247475558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Logistic Regression Classifier\n",
    "clf_bow = LogisticRegression()\n",
    "clf_bow.fit(bow_train, train_df[\"target\"])\n",
    "scores = model_selection.cross_val_score(clf_bow, bow_train, train_df['target'], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "\n",
    "f1_lr_BOW = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.56787565 0.50324977 0.56838366 0.44067797 0.68305531]\n",
      "\n",
      " AVERAGE OF SCORES:  0.5526484704581053\n",
      "\n",
      " STANDARD DEVIATION:  0.08057671249537032\n"
     ]
    }
   ],
   "source": [
    "clf_tfidf = LogisticRegression()\n",
    "clf_tfidf.fit(tfidf_train, train_df[\"target\"])\n",
    "scores = model_selection.cross_val_score(clf_tfidf, tfidf_train, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "\n",
    "f1_lr_TFIDF = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Average score was lower than I expect. I think the naive bayes, while it does better than logistic regression, still underfits the data or perhpas more preprocessing of the text could enable us to use a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.64171975 0.62921348 0.68188105 0.64203233 0.73523557]\n",
      "\n",
      " AVERAGE OF SCORES:  0.666016435892104\n",
      "\n",
      " STANDARD DEVIATION:  0.038892153190544565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Naive Bayes Classifier - BOW\n",
    "clf_nb_bow = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_nb_bow, bow_train, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "f1_nb_BOW = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nb_bow.fit(bow_train,train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.58235294 0.57271095 0.62286689 0.60740741 0.7605178 ]\n",
      "\n",
      " AVERAGE OF SCORES:  0.6291711987321226\n",
      "\n",
      " STANDARD DEVIATION:  0.0680368210871148\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier - TFIDF\n",
    "clf_nb_tfidf = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_nb_tfidf, tfidf_train, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "\n",
    "f1_nb_TFIDF = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nb_tfidf.fit(tfidf_train,train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost did the worse of the three simpler models. I would throw this model out as a candidate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.4894958  0.46654611 0.50652742 0.4305835  0.5470852 ]\n",
      "\n",
      " AVERAGE OF SCORES:  0.48804760567568434\n",
      "\n",
      " STANDARD DEVIATION:  0.03897043988998112\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf_xgb_bow = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(clf_xgb_bow, bow_train, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "\n",
    "f1_xg_BOW = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES:  [0.50108932 0.38640777 0.47644444 0.40082645 0.56071429]\n",
      "\n",
      " AVERAGE OF SCORES:  0.4650964536097499\n",
      "\n",
      " STANDARD DEVIATION:  0.06463679558851756\n"
     ]
    }
   ],
   "source": [
    "clf_xgb_tfidf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(clf_xgb_tfidf, tfidf_train, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(\"SCORES: \", scores)\n",
    "print(\"\\n AVERAGE OF SCORES: \", np.mean(scores))\n",
    "print(\"\\n STANDARD DEVIATION: \", np.std(scores))\n",
    "f1_xg_TFIDF = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "We will calculate the word embeddings and apply it to candidate models Word2Vec, Logistic Regression, SVM, and Random Forest.\n",
    "\n",
    "I tried to reduce the number of tokens fed into the model by removing rare words. There are only about 500 words that appear more then once in the corpus. Removing the remainder would make many empty token arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 968 samples and 12176 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize cleaned text\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['tokens']=train_df['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "test_df['tokens']=test_df['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "train_df['tokens']\n",
    "#freq_dist = nltk.FreqDist(train_df['tokens'])\n",
    "#rarewords = freq_dist.keys()[-5:]\n",
    "\n",
    "#cfdist = ConditionalFreqDist()\n",
    "cfdist = ConditionalFreqDist()\n",
    "for x in train_df['tokens']:\n",
    "    for word in x:\n",
    "        condition = len(word)\n",
    "        cfdist[condition][word] += 1\n",
    "\n",
    "freq_dist = cfdist[3]\n",
    "print(freq_dist)\n",
    "most_common = freq_dist.most_common(500)\n",
    "\n",
    "most_common_list = []\n",
    "for val,freq in most_common:\n",
    "    most_common_list.append(val)\n",
    "    \n",
    "def filter_tokens(tokens):\n",
    "#for x in train_df['tokens']:\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in most_common_list: \n",
    "            filtered_tokens.append(word) \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "#train_df[\"filtered_tokens\"] = train_df['tokens'].apply(filter_tokens)\n",
    "#strain_df[\"tokens\"]\n",
    "#train_df[\"filtered_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1379081, 1469520)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "def fn_pre_process_data(doc):\n",
    "    for rec in doc:\n",
    "        yield gensim.utils.simple_preprocess(rec)\n",
    "\n",
    "corpus = list(fn_pre_process_data(train_df['text_clean']))\n",
    "corpus += list(fn_pre_process_data(train_df['text_clean']))\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv_model = Word2Vec(corpus,size=150,window=3,min_count=2)\n",
    "wv_model.train(corpus,total_examples=len(corpus),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(token_list,vector,k=150):\n",
    "    if len(token_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list] \n",
    "    \n",
    "    sum = np.sum(vectorized,axis=0)\n",
    "    ## return the average\n",
    "    return sum/len(vectorized)        \n",
    "def get_embeddings(tokens,vector):\n",
    "        embeddings = tokens.apply(lambda x: get_word_embeddings(x, wv_model))\n",
    "        return list(embeddings)\n",
    "\n",
    "train_embeddings = get_embeddings(train_df['tokens'],wv_model)\n",
    "test_embeddings = get_embeddings(test_df['tokens'],wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.666679212346296\n",
      "{'C': 10, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "grid_values ={'penalty':['l1','l2'],'C':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "grid_search_model = GridSearchCV(lr_model,param_grid=grid_values,cv=3, scoring=\"f1\")\n",
    "grid_search_model.fit(train_embeddings,train_df['target'])\n",
    "print(grid_search_model.best_estimator_)\n",
    "print(grid_search_model.best_score_)\n",
    "print(grid_search_model.best_params_)\n",
    "\n",
    "f1_lr_W2V = grid_search_model.best_score_\n",
    "\n",
    "predict_lr = grid_search_model.predict(test_embeddings)\n",
    "\n",
    "y_train_pred = grid_search_model.predict(train_embeddings)\n",
    "y_train = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.6442503139675405\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "grid_values ={'C':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "\n",
    "grid_search_model = GridSearchCV(svc_model,param_grid=grid_values,cv=3, scoring=\"f1\")\n",
    "grid_search_model.fit(train_embeddings,train_df['target'])\n",
    "print(grid_search_model.best_estimator_)\n",
    "print(grid_search_model.best_score_)\n",
    "print(grid_search_model.best_params_)\n",
    "\n",
    "f1_svm_W2V = grid_search_model.best_score_\n",
    "\n",
    "predict_svc = grid_search_model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.6697850322024964\n",
      "{'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_values ={'n_estimators':[10,50,100,150,200,300,400]}\n",
    "\n",
    "grid_search_model = GridSearchCV(rf_model,param_grid=grid_values,cv=3, scoring=\"f1\")\n",
    "grid_search_model.fit(train_embeddings,train_df['target'])\n",
    "print(grid_search_model.best_estimator_)\n",
    "print(grid_search_model.best_score_)\n",
    "print(grid_search_model.best_params_)\n",
    "\n",
    "f1_rf_W2V = grid_search_model.best_score_\n",
    "\n",
    "predict_rf = grid_search_model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "The evaluation metric that the Kaggle competition uses is the F1 score. This paragraph is meant to define the F1 score and understand what contributes to this score.\n",
    "\n",
    "According to [deepai.org](https://deepai.org/machine-learning-glossary-and-terms/f-score), the F1 score is the harmonic mean of the precision and recall.\n",
    "\n",
    "[Beyond Accuracy: Precision and Recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c), a blog published on Towards Data Science, authored by Will Koehresen helps us define each piece of the F1 Score.\n",
    "\n",
    "Precision - the proportion of positive results that are truly positive. The calculation is the count of correctly identified disasters divided by every observation labeled as a disaster regardless of correctness. The score is penalized by the incorrectly labeled \"disaster\" observations .\n",
    "\n",
    "Recall - measures the model's ability to identify true positives in a dataset. The true positives are divided by all positive instances, regardless of the model's ability to correctly identify them. This score is penalized by classifying true disasters observations as false (not a disaster).\n",
    "\n",
    "![title](img/recall_df.png)\n",
    "![title](img/precision_df.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recall(true_positives, false_negatives):\n",
    "    return (true_positives)/(true_positives + false_negatives)\n",
    "def _precision(true_positives, false_positives):\n",
    "    return (true_positives)/(true_positives + false_positives)\n",
    "def _f1(precision, recall):\n",
    "    return (precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "\n",
      "\n",
      "IF ALL OBSERVATIONS WERE SET TO DISASTER (POSITIVE)\n",
      "RECALL:  1.0\n",
      "PRECISION:  0.4165243662156837\n",
      "F1:  0.2940467359050445\n",
      "\n",
      "\n",
      "IF ALL OBSERVATIONS WERE SET TO NOT DISASTER (NEGATIVE)\n",
      "RECALL: 0 \n",
      "PRECISION: 0\n",
      "F1: 0\n",
      "\n",
      "\n",
      "IF ALL OBSERVATIONS WERE SET TO NOT DISASTER (NEGATIVE) But 1 correctly identified true positive\n",
      "RECALL:  0.00013135426244581636\n",
      "PRECISION:  1.0\n",
      "F1:  0.00013133701076963486\n"
     ]
    }
   ],
   "source": [
    "print(train_df['target'].value_counts())\n",
    "\n",
    "true_positives = 3171\n",
    "total_observations = 7613\n",
    "# If we were to hypothetically set all observations to DISASTER\n",
    "false_positives = total_observations-true_positives\n",
    "recall = _recall(true_positives,0)\n",
    "precision = _precision(true_positives,false_positives)\n",
    "print('\\n\\nIF ALL OBSERVATIONS WERE SET TO DISASTER (POSITIVE)')\n",
    "print('RECALL: ', recall)\n",
    "print('PRECISION: ', precision)\n",
    "print('F1: ', _f1(precision, recall))\n",
    "\n",
    "# If we were to hypothetically set all observations to NOT A DISASTER\n",
    "print('\\n\\nIF ALL OBSERVATIONS WERE SET TO NOT DISASTER (NEGATIVE)')\n",
    "print('RECALL: 0 ')\n",
    "print('PRECISION: 0')\n",
    "print('F1: 0')\n",
    "\n",
    "# If we were to hypothetically set all observations to NOT A DISASTER but correctly identified 1 disaster\n",
    "false_negatives = total_observations - 1\n",
    "true_positives = 1\n",
    "false_positives = 0\n",
    "recall = _recall(true_positives,false_negatives)\n",
    "precision = _precision(true_positives,false_positives)\n",
    "print('\\n\\nIF ALL OBSERVATIONS WERE SET TO NOT DISASTER (NEGATIVE) But 1 correctly identified true positive')\n",
    "print('RECALL: ', recall)\n",
    "print('PRECISION: ', precision)\n",
    "print('F1: ', _f1(precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above findings: If we label all tweets as positive disasters, then we have a high recall because we identified all disasters, but a low precision because we also incorrectly labeled many non disasters as disasters. In contrast, if we identified one true positive, but labeled the rest negatives, then the precision is high because we didn't incorrectly identify any non disasters as disasters, but recall is low because we missed many true disasters.\n",
    "\n",
    "\n",
    "Thinking about this from a cost/benefit perspective, if every tweet was labeled as a disaster (high recall, low precision), then the resources used to report and investigate the disaster is very costly. If ever tweet was labeled no disaster, then there is no benefit from using the model. Many true disasters are missed, which could cost the organization in their reputation from having a delayed reaction time to true emergencies. In this situation a true balance between recall and precision is desired.\n",
    "\n",
    "A higher F1 score represents optimizing the balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores - BOW:\n",
      "\n",
      "Logistic Regression:  0.6047830525449209\n",
      "\n",
      "Naive Bayes:  0.666016435892104\n",
      "\n",
      "XGBoost 0.48804760567568434\n",
      "\n",
      "F1 Scores - TFIDF\n",
      "\n",
      "Logistic Regression:  0.5526484704581053\n",
      "\n",
      "Naive Bayes:  0.6291711987321226\n",
      "\n",
      "XGBoost 0.4650964536097499\n",
      "\n",
      "F1 Scores - Word Embeddings\n",
      "\n",
      "Logistic Regression:  0.666679212346296\n",
      "\n",
      "Support Machine Vectors:  0.6442503139675405\n",
      "\n",
      "Random Forest:  0.6697850322024964\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Scores - BOW:\")\n",
    "print(\"\\nLogistic Regression: \", f1_lr_BOW)\n",
    "print(\"\\nNaive Bayes: \", f1_nb_BOW)\n",
    "print(\"\\nXGBoost\", f1_xg_BOW)\n",
    "print(\"\\nF1 Scores - TFIDF\")\n",
    "print(\"\\nLogistic Regression: \", f1_lr_TFIDF)\n",
    "print(\"\\nNaive Bayes: \", f1_nb_TFIDF)\n",
    "print(\"\\nXGBoost\", f1_xg_TFIDF)\n",
    "print(\"\\nF1 Scores - Word Embeddings\")\n",
    "print(\"\\nLogistic Regression: \", f1_lr_W2V)\n",
    "print(\"\\nSupport Machine Vectors: \", f1_svm_W2V)\n",
    "print(\"\\nRandom Forest: \", f1_rf_W2V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.7860326894502229\n",
      "recall:  0.6468969734026292\n",
      "accuracy:  0.7726257717062919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,accuracy_score\n",
    "\n",
    "print(\"precision: \", precision_score(y_train,y_train_pred))\n",
    "print(\"recall: \", recall_score(y_train,y_train_pred))\n",
    "print(\"accuracy: \", accuracy_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CSV(df,file_name):\n",
    "    file_name = file_name+\".csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    \n",
    "#submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\n",
    "#test_vectors=test_tfidf\n",
    "#submission(submission_file_path,clf_NB_TFIDF,test_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
